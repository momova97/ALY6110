{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgTF5/TpVDuBvHVWScxoJm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/momova97/ALY6110/blob/main/Assignment_Week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment week 5\n",
        "By Mohammad Movahedi"
      ],
      "metadata": {
        "id": "4l6FjC0X1DRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#introduction\n",
        "This assignment delves into the realm of Big Data Processing using Apache Spark, focusing on a Word Count application applied to the \"Meditation\" text from the Gothenburg Project, all within the Google Colab environment. Following the comprehensive instructions outlined in the \"Big Data Processing with Apache Spark – Part 1: Introduction,\" I installed the Java Development Kit (JDK) and Apache Spark. Subsequently, I adapted the Word Count application to analyze the contents of the \"Meditation\" text. This assignment is a practical exploration of Apache Spark's capabilities for processing extensive datasets and extracting meaningful insights from textual content."
      ],
      "metadata": {
        "id": "RRjs0h8TJPjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "In this part I will perform the following tasks:\n",
        "- Download the text file from Project Gutenberg\n",
        "- Read the text file into an RDD\n",
        "- Perform Word Count\n",
        "- Find the Most Common Word\n",
        "- Find the Word with the Fewest Occurrences"
      ],
      "metadata": {
        "id": "iIwwW_wsKMUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment installs Apache Spark in Google Colab. It begins by installing the OpenJDK 8 JDK (Java Development Kit) and then downloads and extracts the Spark distribution (version 3.1.2) with Hadoop 2.7. Finally, the findspark library is installed using pip. findspark is a Python library that allows the integration of Spark with Jupyter Notebooks or other Python environments. This setup enables the use of Apache Spark for distributed data processing in a Google Colab environment."
      ],
      "metadata": {
        "id": "4UzmLhbZKsnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Spark in Google Colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "5qmANKsS1KUW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This sets environment variables for Java and Spark in the Google Colab environment. Specifically, it designates the Java Home path as \"/usr/lib/jvm/java-8-openjdk-amd64\" and the Spark Home path as \"/content/spark-3.1.2-bin-hadoop2.7\". These environment variables are crucial for ensuring that the Python environment can locate and communicate with the Java and Spark installations."
      ],
      "metadata": {
        "id": "YwrlmI09K03X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "TYetUYbS1eUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " the findspark library. findspark is a Python library that helps Python programs locate Spark's installation on a system, facilitating the integration of Spark with Python environments like Jupyter Notebooks. By running findspark.init(), it configures the necessary environment variables to enable the seamless use of Spark within the Python environment in Google Colab."
      ],
      "metadata": {
        "id": "YgSNu4vYK-H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install FindSpark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "qHaR_qNZ1hSb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a SparkSession named spark by invoking SparkSession.builder.master(\"local[*]\").getOrCreate(). The specified master URL, \"local[*]\", indicates that Spark should run in local mode using all available cores on the machine. This Spark session serves as the entry point for interacting with Spark functionalities, enabling distributed data processing."
      ],
      "metadata": {
        "id": "sSQar1l3LG6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "Nvm6AiLp1q4t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the text content of a meditation text from Project Gutenberg using the requests library. The specified URL points to the text file, and the content is retrieved using the get method. Subsequently, the obtained text content is saved to a local file named \"sample.txt\" in UTF-8 encoding."
      ],
      "metadata": {
        "id": "4Nw0QP9pLF75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the meditation text from Project Gutenberg\n",
        "url = \"https://www.gutenberg.org/cache/epub/2680/pg2680.txt\"\n",
        "\n",
        "# Download the text file\n",
        "response = requests.get(url)\n",
        "text_content = response.text\n",
        "\n",
        "# Save the content to a local file named sample.txt\n",
        "with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text_content)\n",
        "\n",
        "# Print a message indicating the successful download and save\n",
        "print(\"Meditation text downloaded from Project Gutenberg and saved as sample.txt.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8leulkJd1zVc",
        "outputId": "61770180-ec96-47e3-d418-3aba763780b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meditation text downloaded from Project Gutenberg and saved as sample.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code performs a basic Word Count analysis using Apache Spark in Google Colab. It reads the text file \"sample.txt\" into a Resilient Distributed Dataset (RDD) using Spark's textFile method. The Word Count operation is then carried out by first using flatMap to split each line into words, followed by map to associate each word with the count of 1. The reduceByKey operation is then employed to aggregate the counts for each word. The final results are collected and printed, displaying the count for each unique word in the provided text."
      ],
      "metadata": {
        "id": "HwUaN5mqQIY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file into an RDD\n",
        "text_data = spark.sparkContext.textFile(\"sample.txt\")\n",
        "\n",
        "# Perform Word Count\n",
        "word_count_data = text_data.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the Word Count Results\n",
        "result = word_count_data.collect()\n",
        "\n",
        "# Display the Word Count Results\n",
        "for (word, count) in result:\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "id": "1F71arc513n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code identifies the most common word and its occurrence count from the Word Count results obtained earlier. The max function is used with a lambda function to find the element with the maximum count in the word_count_data RDD. The result is then printed, revealing the most common word in the provided text (\"and\") and its frequency (3149 occurrences)"
      ],
      "metadata": {
        "id": "DjpGoScYQdFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Most Common Word\n",
        "most_common_word = word_count_data.max(lambda x: x[1])\n",
        "\n",
        "# Display the Most Common Word and its Occurrence Count\n",
        "print(f\"The most common word is '{most_common_word[0]}' with {most_common_word[1]} occurrences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY6YIO4w8VBP",
        "outputId": "5954fe0f-aa7f-4ce7-bae8-9fe782dd473a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most common word is 'and' with 3149 occurrences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code  identifies the word with the fewest occurrences and its count from the Word Count results obtained earlier. The min function is used with a lambda function to find the element with the minimum count in the word_count_data RDD. The result is then printed, revealing the word with the fewest occurrences in the provided text (\"Author:\") and its frequency (1 occurrence)."
      ],
      "metadata": {
        "id": "1bniZpsXQn29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Word with the Fewest Occurrences\n",
        "fewest_occurrences_word = word_count_data.min(lambda x: x[1])\n",
        "\n",
        "# Display the Word with the Fewest Occurrences and its Occurrence Count\n",
        "print(f\"The word with the fewest occurrences is '{fewest_occurrences_word[0]}' with {fewest_occurrences_word[1]} occurrences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3GnZKHm4D80",
        "outputId": "b9726999-246e-4b61-fbac-120f59d02cf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word with the fewest occurrences is 'Author:' with 1 occurrences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code introduces a timing mechanism to measure the execution duration of the Word Count job using Apache Spark. The time module is employed to record the start time before executing the Word Count operation and the end time afterward. The elapsed time is then calculated by subtracting the start time from the end time. The Word Count results are displayed, followed by the elapsed time, providing insights into the computational efficiency of the Spark job. In this specific example, the Word Count job took approximately 0.04 seconds to complete."
      ],
      "metadata": {
        "id": "C6NKuD8pQ2QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Start the Timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform Word Count\n",
        "word_count_data = text_data.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Stop the Timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the Elapsed Time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Display the Word Count Results\n",
        "#result = word_count_data.collect()\n",
        "#for (word, count) in result:\n",
        "#    print(f\"{word}: {count}\")\n",
        "\n",
        "# Display the Elapsed Time\n",
        "print(f\"\\nThe word count job took {elapsed_time:.2f} seconds to complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV996NHE8-7P",
        "outputId": "36aa808c-4786-40d3-9dfe-55fb94033396"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The word count job took 0.04 seconds to complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conclusion\n",
        "In conclusion, our journey through this assignment provided a hands-on experience with Apache Spark, tailored to the unique context of processing the \"Meditation\" text. By adapting the Word Count application, we gained valuable insights into the frequency of words, identified the most and least common terms, and measured the time taken for the word count job, all within the Google Colab environment. This practical exercise not only solidified our understanding of Apache Spark's functionality but also demonstrated its applicability to diverse datasets. As we continue our exploration of data analytics, this assignment stands as a foundational step in leveraging powerful tools for efficient processing and analysis of substantial textual datasets."
      ],
      "metadata": {
        "id": "lfjybkIpKY-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reference\n",
        "Srini Penchikala (2015). Big Data Processing with Apache Spark – Part 1: Introduction. [online] InfoQ. Available at: https://www.infoq.com/articles/apache-spark-introduction/ [Accessed 4 Dec. 2023].\n",
        "\n",
        "‌"
      ],
      "metadata": {
        "id": "ASKxq8EyQ4qW"
      }
    }
  ]
}